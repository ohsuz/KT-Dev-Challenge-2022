{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using Mecab\n",
    "# !curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n",
    "# !pip install -U \"jpype1<1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "fix_seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from deep_utils import stratify_train_test_split_multi_label\n",
    "import copy\n",
    "\n",
    "\n",
    "class T5NERDataset(Dataset):\n",
    "    def __init__(self, args, data_name, tokenizer, kfold_idx = None):\n",
    "        self.args = args\n",
    "        self.data_name = data_name\n",
    "        self.max_len = args.max_len\n",
    "        self.batch_size = args.batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eng_to_kor = {'PS': '사람', 'LC': '위치', 'OG': '기관', 'DT': '날짜', 'TI': '시간', 'QT': '수량'}\n",
    "        \n",
    "        if data_name in ['train', 'val']:\n",
    "            data_path = os.path.join(args.data_path, 'klue_ner_train_80.txt')\n",
    "        else:\n",
    "            data_path = os.path.join(args.data_path, 'klue_ner_test_20.txt')\n",
    "        f = open(data_path)\n",
    "        self.raw_data = f.readlines()\n",
    "        self.raw_df = self._prepare_df(self.raw_data)\n",
    "        self.original_df = copy.deepcopy(self.raw_df)\n",
    "        # ----- Train : Val Split ------- #\n",
    "        self.raw_df[\"id\"] = self.raw_df.index\n",
    "        self.raw_df[\"y\"] = self.raw_df[\"tags\"].apply(lambda x : convert_tags_to_vector(x))\n",
    "        y = np.array([np.array(ls) for ls in self.raw_df[\"y\"]])\n",
    "        train_X, test_X, train_y, test_y = stratify_train_test_split_multi_label(self.raw_df[\"id\"], y, test_size=args.val_ratio)\n",
    "        if data_name == 'train':\n",
    "            self.raw_df = self.raw_df.loc[train_X]\n",
    "        if data_name == 'val':\n",
    "            self.raw_df = self.raw_df.loc[test_X]\n",
    "        # -------------------------------- #\n",
    "        \n",
    "        self._set_input_text()\n",
    "        print(len(self.raw_df), len(self.input_text))\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self._preprocess(self.input_text[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def _preprocess(self, input_text) :\n",
    "        encoder_text, decoder_target = input_text\n",
    "        encoder_inputs = self.tokenizer(encoder_text, max_length = self.args.max_len, padding = \"max_length\", truncation = True, return_tensors = 'pt')\n",
    "        decoder_inputs = self.tokenizer(decoder_target, max_length = self.args.max_len, padding = \"max_length\", truncation = True, return_tensors = 'pt')\n",
    "        \n",
    "        src_ids, src_mask = encoder_inputs[\"input_ids\"], encoder_inputs[\"attention_mask\"]\n",
    "        tgt_ids = decoder_inputs[\"input_ids\"]\n",
    "        return {'src_ids': src_ids.squeeze(), 'src_mask': src_mask.squeeze(), 'tgt_ids': tgt_ids.squeeze()}\n",
    "\n",
    "    def _get_special_tokens(self, args) :\n",
    "        self.sep = self.tokenizer.sep_token\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.input_pad_id = self.tokenizer.pad_token_id\n",
    "        self.target_pad_id = -1004\n",
    "    \n",
    "    def _set_input_text(self) :\n",
    "        ner_inputs, ner_outputs = self._get_ner_inputs()\n",
    "        if self.data_name in  ['val', 'test']:\n",
    "            self.input_text = [(enc, dec) for enc, dec in zip(ner_inputs, ner_outputs)]\n",
    "            return\n",
    "        ee_inputs, ee_outputs = self._get_ee_inputs()\n",
    "        et_inputs, et_outputs = self._get_et_inputs()\n",
    "        pos_inputs, pos_outputs = self._get_pos_inputs()\n",
    "        hmn_inputs, hmn_outputs = self._get_hmn_inputs()\n",
    "        inputs, outputs = ner_inputs+ee_inputs+et_inputs+hmn_inputs, ner_outputs+ee_outputs+et_outputs+hmn_outputs\n",
    "        self.input_text = [(enc, dec) for enc, dec in zip(inputs, outputs)]\n",
    "    \n",
    "    def _preprocess_text(self, line):\n",
    "        entities, tags = [], []\n",
    "        l = re.findall(r'<[%-=+,#/\\?:^.@*\\\"※~ㆍ!』‘|\\(\\)\\[\\]`\\'…》\\”\\“\\’·\\s0-9ㄱ-ㅣ가-힣A-Za-z]+:[A-Za-z]+>', line)\n",
    "\n",
    "        for label in l:\n",
    "            entity, tag = label.replace('<', '').replace('>','').split(':')\n",
    "            entities.append(entity)\n",
    "            tags.append(tag)\n",
    "            line = line.replace('\\n', '').replace(label, entity)\n",
    "    \n",
    "        return line, entities, tags\n",
    "\n",
    "    def _prepare_df(self, data):\n",
    "        preprocessed_text, entities, tags, counts = [], [], [], []\n",
    "\n",
    "        for line in data:\n",
    "            line, entity, tag = self._preprocess_text(line)\n",
    "            preprocessed_text.append(line)\n",
    "            entities.append(entity)\n",
    "            tags.append(tag)\n",
    "            counts.append(len(entity))\n",
    "\n",
    "        df = pd.DataFrame({\"text\": data, 'preprocessed_text': preprocessed_text, 'entities': entities, 'tags': tags, 'cnt': counts})\n",
    "        return df\n",
    "    \n",
    "    def _get_ner_inputs(self):\n",
    "        inputs, outputs = [], []\n",
    "        entities, tags = list(self.raw_df['entities']), list(self.raw_df['tags'])\n",
    "        instruction = ' Instruction: Input Sentence에서 찾을 수 있는 모든 Entity 및 그들의 Entity type을 출력하세요.'+\\\n",
    "                        ' 가능한 Entity type은 다음과 같습니다: 사람, 위치, 기관, 날짜, 시간, 수량'\n",
    "\n",
    "        for i, text in enumerate(list(self.raw_df['preprocessed_text'])):\n",
    "            input_sentence = 'Sentence: ' + text + instruction\n",
    "            output_sentence = ''\n",
    "            kor_tags = [self.eng_to_kor[tag] for tag in tags[i]]\n",
    "            for idx, entity in enumerate(entities[i]):\n",
    "                output_sentence += (entity+get_josa(entity, 'ent')+' ')\n",
    "                if idx == len(entities[i])-1:\n",
    "                    output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'다.')\n",
    "                else:\n",
    "                    output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'고, ')\n",
    "            inputs.append(input_sentence)\n",
    "            outputs.append(output_sentence)\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def _get_ee_inputs(self):\n",
    "        inputs, outputs = [], []\n",
    "        entities, tags = list(self.raw_df['entities']), list(self.raw_df['tags'])\n",
    "        instruction = ' Instruction: Input Sentence에서 Entity에 해당하는 단어를 모두 출력하세요.'\n",
    "\n",
    "        for i, text in enumerate(list(self.raw_df['preprocessed_text'])):\n",
    "            input_sentence = 'Sentence: ' + text + instruction\n",
    "            output_sentence = ', '.join(entities[i]) + '.'\n",
    "            inputs.append(input_sentence)\n",
    "            outputs.append(output_sentence)\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def _get_et_inputs(self):\n",
    "        inputs, outputs = [], []\n",
    "        entities, tags = list(self.raw_df['entities']), list(self.raw_df['tags'])\n",
    "\n",
    "        for i, text in enumerate(list(self.raw_df['preprocessed_text'])):\n",
    "            input_entities = ', '.join(entities[i])\n",
    "            input_sentence = 'Sentence: ' + text + ' Instruction: Input Sentence에서 <' + input_entities + \\\n",
    "                                '>의 Entity type을 출력하세요. 가능한 Entity type은 다음과 같습니다: 사람, 위치, 기관, 날짜, 시간, 수량'\n",
    "            output_sentence = ''\n",
    "            kor_tags = [self.eng_to_kor[tag] for tag in tags[i]]\n",
    "            for idx, entity in enumerate(entities[i]):\n",
    "                output_sentence += (entity+get_josa(entity, 'ent')+' ')\n",
    "                if idx == len(entities[i])-1:\n",
    "                    output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'다.')\n",
    "                else:\n",
    "                    output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'고, ')\n",
    "            inputs.append(input_sentence)\n",
    "            outputs.append(output_sentence)\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def _get_hmn_inputs(self):\n",
    "        inputs, outputs = [], []\n",
    "        texts, entities, tags = list(self.original_df['preprocessed_text']), list(self.original_df['entities']), list(self.original_df['tags'])\n",
    "        instruction = 'Instruction: Sentence1과 Sentence2에서 Entity에 해당하는 단어와 그들의 Entity type을 출력하세요.'\n",
    "        ent_dict = defaultdict(list)\n",
    "        for i, entity in enumerate(entities):\n",
    "            for idx, ent in enumerate(entity):\n",
    "                ent_dict[ent].append((tags[i][idx], texts[i]))\n",
    "                \n",
    "        ent_dict_del_1 = defaultdict(list)\n",
    "        for key in ent_dict.keys():\n",
    "            if len(ent_dict[key]) != 1:\n",
    "                ent_dict_del_1[key] = ent_dict[key]\n",
    "                \n",
    "        for key in ent_dict_del_1.keys():\n",
    "            sents = []\n",
    "            for item1, item2 in list(combinations(ent_dict_del_1[key], 2)):\n",
    "                if item1[0] != item2[0]:\n",
    "                    if item1[1] not in sents and item2[1] not in sents:\n",
    "                        sents.append(item1[1])\n",
    "                        inputs.append(f'Sentence1: {item1[1]} Sentence2: {item2[1]} {instruction}')\n",
    "                        kor_tags = [self.eng_to_kor[tag] for tag in [item1[0], item2[0]]]\n",
    "                        output_sentence = ''\n",
    "                        for idx in range(2):\n",
    "                            output_sentence += (key+get_josa(key, 'ent')+' ')\n",
    "                            if idx == 1:\n",
    "                                output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'다.')\n",
    "                            else:\n",
    "                                output_sentence += (kor_tags[idx]+get_josa(kor_tags[idx], 'tag')+'고, ')\n",
    "                        outputs.append(output_sentence)\n",
    "        return inputs, outputs\n",
    "        \n",
    "\n",
    "    def _get_pos_inputs(self):\n",
    "        inputs, outputs = [], []\n",
    "        entities, pos_tags = list(self.raw_df['entities']), self.get_pos_tags()\n",
    "        \n",
    "        instruction = ' Instruction: Input Sentence에서 찾을 수 있는 모든 Entity 및 그들의 품사를 출력하세요.'+\\\n",
    "                        ' 가능한 품사는 다음과 같습니다: 일반명사, 고유명사, 단위명사, 수사, 해당없음'\n",
    "\n",
    "        for i, text in enumerate(list(self.raw_df['preprocessed_text'])):\n",
    "            input_sentence = 'Sentence: ' + text + instruction\n",
    "            output_sentence = ''\n",
    "            pos_tag = pos_tags[i]\n",
    "            for idx, entity in enumerate(entities[i]):\n",
    "                output_sentence += (entity+get_josa(entity, 'ent')+' ')\n",
    "                if idx == len(entities[i])-1:\n",
    "                    output_sentence += (pos_tag[idx]+get_josa(pos_tag[idx], 'tag')+'다.')\n",
    "                else:\n",
    "                    output_sentence += (pos_tag[idx]+get_josa(pos_tag[idx], 'tag')+'고, ')\n",
    "            inputs.append(input_sentence)\n",
    "            outputs.append(output_sentence)\n",
    "        \n",
    "        return inputs, outputs\n",
    "    \n",
    "    def get_pos_tags(self):\n",
    "        from konlpy.tag import Mecab\n",
    "        mecab = Mecab()\n",
    "        NE_pos_dict = {'NNG':'일반명사', 'NNP':'고유명사', 'NNBC': '단위명사', 'NR': '수사'}\n",
    "        total_pos_of_entities = []\n",
    "        for ent in list(self.raw_df['entities']):\n",
    "            pos_of_entities = []\n",
    "            for e in ent:\n",
    "                pos = [pos for tok, pos in mecab.pos(e)]\n",
    "                candidates = Counter(pos).most_common()\n",
    "                others = True\n",
    "                if 'NR' in pos:  \n",
    "                    pos_of_entities.append(NE_pos_dict['NR'])\n",
    "                    others = False\n",
    "                elif 'SN' in pos or 'NNBC' in pos:\n",
    "                    pos_of_entities.append(NE_pos_dict['NNBC'])\n",
    "                    others = False\n",
    "                else:\n",
    "                    for candidate, _ in candidates:\n",
    "                        if candidate in list(NE_pos_dict.keys()):\n",
    "                            pos_of_entities.append(NE_pos_dict[candidate])\n",
    "                            others = False\n",
    "                            break\n",
    "                if others == True:\n",
    "                    pos_of_entities.append('해당없음')\n",
    "            total_pos_of_entities.append(pos_of_entities)\n",
    "        return total_pos_of_entities\n",
    "\n",
    "\n",
    "def get_josa(s, s_type): \n",
    "    NO_JONGSUNG = 'ᴕ'\n",
    "    CHOSUNGS = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    JOONGSUNGS = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    JONGSUNGS = [NO_JONGSUNG,  'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    N_CHOSUNGS, N_JOONGSUNGS, N_JONGSUNGS = 19, 21, 28\n",
    "    FIRST_HANGUL, LAST_HANGUL = 0xAC00, 0xD7A3 #'가', '힣'\n",
    "    \n",
    "    result = []\n",
    "    for c in s:\n",
    "        if ord(c) < FIRST_HANGUL or ord(c) > LAST_HANGUL: # if a character is a hangul\n",
    "            result.append(c)\n",
    "        else:            \n",
    "            code = ord(c) - FIRST_HANGUL\n",
    "            jongsung_index = code % N_JONGSUNGS\n",
    "            code //= N_JONGSUNGS\n",
    "            joongsung_index = code % N_JOONGSUNGS\n",
    "            code //= N_JOONGSUNGS\n",
    "            chosung_index = code\n",
    "\n",
    "            result.append(CHOSUNGS[chosung_index])\n",
    "            result.append(JOONGSUNGS[joongsung_index])\n",
    "            result.append(JONGSUNGS[jongsung_index])\n",
    "    jaso_str = ''.join(result)\n",
    "    if s_type == 'ent':\n",
    "        josa = '는' if jaso_str[-1] == 'ᴕ' else '은'  # 종성 없으면 '는' 있으면 '은'\n",
    "    if s_type == 'tag':\n",
    "        josa = '' if jaso_str[-1] == 'ᴕ' else '이'  # 종성 없으면 '' 있으면 '이' \n",
    "    return josa\n",
    "\n",
    "\n",
    "def convert_tags_to_vector(_tags):\n",
    "    labels = [0]*6\n",
    "    for tag in _tags:\n",
    "        if tag == 'QT':\n",
    "            labels[0] += 1\n",
    "        if tag == 'DT':\n",
    "            labels[1] += 1\n",
    "        if tag == 'PS':\n",
    "            labels[2] += 1\n",
    "        if tag == 'LC':\n",
    "            labels[3] += 1\n",
    "        if tag == 'TI':\n",
    "            labels[4] += 1\n",
    "        if tag == 'OG':\n",
    "            labels[5] += 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "  val_ratio=0.2,\n",
    "  data_path='data_learn',\n",
    "  max_len=128,\n",
    "  batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'model/kt-ulm-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16761 54795\n"
     ]
    }
   ],
   "source": [
    "train_dataset = T5NERDataset(args=args, data_name='train', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4041 4041\n"
     ]
    }
   ],
   "source": [
    "val_dataset = T5NERDataset(args=args, data_name='val', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5201 5201\n"
     ]
    }
   ],
   "source": [
    "test_dataset = T5NERDataset(args=args, data_name='test', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': args.batch_size,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 2\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': args.batch_size,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 2\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params)\n",
    "val_loader = DataLoader(val_dataset, **val_params)\n",
    "test_loader = DataLoader(test_dataset, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    train_total_loss = 0\n",
    "    for idx, data in enumerate(tqdm(loader, 0)):\n",
    "        tgt = data['tgt_ids'].to(device, dtype = torch.long)\n",
    "        tgt_ids = tgt[:, :-1].contiguous()  # eos token 제외\n",
    "        lm_labels = tgt[:, 1:].clone().detach()  # start token(</s>) 제외\n",
    "        lm_labels[tgt[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        src_ids = data['src_ids'].to(device, dtype = torch.long)\n",
    "        src_mask = data['src_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=src_ids, attention_mask=src_mask, decoder_input_ids=tgt_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        train_total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_mean_loss = train_total_loss / (len(loader)*args.batch_size)\n",
    "    wandb.log({\"Epoch\": epoch, \"Train Loss\": train_mean_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(tqdm(loader, 0)):\n",
    "            tgt_ids = data['tgt_ids'].to(device, dtype = torch.long)\n",
    "            src_ids = data['src_ids'].to(device, dtype = torch.long)\n",
    "            src_mask = data['src_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = src_ids,\n",
    "                attention_mask = src_mask, \n",
    "                max_length=128, \n",
    "                num_beams=3,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in tgt_ids]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohsuz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/team09/wandb/run-20220929_142603-3abuspy9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ohsuz/KT/runs/3abuspy9\" target=\"_blank\">InstructionNER+28+Last</a></strong> to <a href=\"https://wandb.ai/ohsuz/KT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ohsuz/KT/runs/3abuspy9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fefa1a663a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='KT', entity=\"ohsuz\", name='InstructionNER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e149ef04528c48b785d82ccb9aaa3a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573a9db2f06a4c809e578d8570777abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c853f2d04a5474796398ae147c733f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b968e73d9c434af4aaa4277ab58a1b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2122ff9bf841fbbb91fd4f8f0e6dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b25337c8044cae8671a228250e88f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae348f95e14400292e61b1fc4098062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1913f0c39dcf44e18660e58383461324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d1da64c8b642a39f7e6fd492b21247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109ff4348f08464588a13d3a9f1db36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 done\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "predictions_per_epoch = {}\n",
    "actuals_per_epoch = {}\n",
    "\n",
    "for epoch in range(50):\n",
    "    train(epoch, tokenizer, model, device, train_loader, optimizer)\n",
    "    # if (epoch + 1) >= 30:\n",
    "    #     if (epoch + 1) % 10 == 0:\n",
    "    #         predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
    "    #         predictions_per_epoch[epoch+1] = predictions\n",
    "    #         actuals_per_epoch[epoch+1] = actuals\n",
    "    #         epoch_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    #         epoch_df.to_csv(f'./baseline_mhn_e{epoch+1}.csv', index=False)\n",
    "    #         torch.save(model.state_dict(), f'baseline_mhn_e{epoch+1}.pt')\n",
    "    #         # epoch_df.to_csv(f'./val_e{epoch+1}.csv', index=False)\n",
    "    #         # torch.save(model.state_dict(), f'model_e{epoch+1}.pt')\n",
    "    print(f'epoch {epoch+1} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52514cfe55ce4b6192d6e810b306c7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, actuals = validate(tokenizer, model, device, test_loader)\n",
    "final_df = pd.DataFrame({'Generated Text':predictions, 'Actual Text':actuals})\n",
    "final_df.to_csv('./output/submission.csv', index=False)\n",
    "torch.save(model.state_dict(), f'./output/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.13 (NGC 22.05/Python 3.8 Conda) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
